#!/usr/bin/env python
"""Process empathic accuracy behavioural data.

Usage:
    dm-proc-ea-behaviour.py [options] <study>
    dm-proc-ea-behaviour.py [options] <study> <session>


Arguments:
    <study>             Study to process
    <outputdir>         Path to output folder
    <session>           Recording session to process

Options:
    -h --help                   Show this screen.
    -q --quiet                  Suppress output.
    -v --verbose                Show more output.
    -d --debug                  Show lots of output.
    -o --output                 Directory for output
    --logdir=<logdir>           Directory to store generated logs
                                    [default: /archive/logs/dm-proc-ea-behaviour]
    --walltime TIME             Walltime for each session job [default: 4:00:00]
    --dryrun                    Perform a dryrun, only observed when session is not provided


"""
import sys
import logging
import os
import sys
import glob
import copy
import time
import tempfile
import shutil
import numpy as np
import StringIO as io
import scipy.interpolate as interpolate
import datman as dm
from docopt import docopt

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

datadir = None

def check_complete(outputdir, session):
    """Checks to see if the output files have been created.
    Returns True if the files exist
    """
    expected_files = ['{}_vid_block-times_ea.1D',
                      '{}_vid_corr_push.csv',
                      '{}_vid_button-times.csv',
                      '{}_vid_vid-onsets.csv',
                      '{}_cvid_block-times_ea.1D',
                      '{}_cvid_corr_push.csv',
                      '{}_cvid_button-times.csv',
                      '{}_cvid_vid-onsets.csv']

    all_found = True
    for filename in expected_files:
        if not os.path.isfile(os.path.join(outputdir, session, \
                                filename.format(session))):
            all_found = False
    return(all_found)

def log_parser(log):
    """
    This takes the EA task log file generated by e-prime and converts it into a
    set of numpy-friendly arrays (with mixed numeric and text fields.)

    pic -- 'Picture' lines, which contain the participant's ratings.
    res -- 'Response' lines, which contain their responses (unclear)
    vid -- 'Video' lines, which demark the start and end of trials.
    """
    # substitute for GREP -- finds 'eventtype' field.
    # required as this file has a different number of fields per line
    logname = copy.copy(log)
    log = open(log, "r").readlines()
    pic = filter(lambda s: 'Picture' in s, log)
    vid = filter(lambda s: 'Video' in s, log)

    # write out files from stringio blobs into numpy genfromtxt
    pic = np.genfromtxt(io.StringIO(''.join(pic)), delimiter='\t',
        names=['subject', 'trial', 'eventtype', 'code', 'time', 'ttime', 'uncertainty1', 'duration', 'uncertainty2', 'reqtime', 'reqduration', 'stimtype', 'pairindex'],
        dtype=['|S64'   , int    , '|S64'     , '|S64', int   , int    , int           , int       , int           , int      , int          , '|S64'    , int])

    vid = np.genfromtxt(io.StringIO(''.join(vid)), delimiter='\t',
        names=['subject', 'trial', 'eventtype', 'code', 'time', 'ttime', 'uncertainty1'],
        dtype=['|S64'   , int    , '|S64'     , '|S64', int   , int    , int])

    # ensure our inputs contain a 'MRI_start' string.
    if pic[0][3] != 'MRI_start':
        logger.error('log {} does not contain an MRI_start entry!'.format(logname))
        raise ValueError
    else:
        # this is the start of the fMRI run, all times are relative to this.
        mri_start = pic[0][7]
        return pic, vid, mri_start

def find_blocks(vid, mri_start):
    """
    Takes the start time and a vid tuple list to find the relative
    block numbers, their start times, and their type (string).
    """
    blocks = []
    onsets = []
    for v in vid:

        # we will use this to search through the response files
        block_number = v[1]

        # this is maybe useless (e.g., 'vid_4')
        block_name = v[3]

        # all time in 10000s of a sec.
        block_start = (v[4])

        # generate compressed video list
        blocks.append((block_number, block_name, block_start))
        onsets.append(block_start / 10000.0)

    return blocks, onsets

def find_ratings(pic, blk_start, blk_end, blk_start_time, duration):
    """
    Takes the response and picture tuple lists and the beginning of the current
    and next videos. This will search through all of the responses [vid_start
    < x < vid_end] and grab their timestamps. For each, it will find the
    corresponding picture rating and save that as an integer.

    All times in 10,000s of a second.

    102,103 -- person responses
    104     -- MRI responses
    """

    ratings = []
    pushes = []
    if blk_end == None:
        # find the final response number, take that as the end of our block
        trial_list = np.linspace(blk_start, pic[-1][1], pic[-1][1]-blk_start+1)
    else:
        # just use the beginning of the next block as our end.
        trial_list = np.linspace(blk_start, blk_end-1, blk_end-blk_start)

    # refine trial list to include only the first, last, and button presses
    responses = np.array(filter(lambda s: s[1] in trial_list, pic))
    responses = np.array(filter(lambda s: 'rating' in s[3], responses))

    # if the participant dosen't respond at all, freak out.
    if len(responses) == 0:
        ratings = np.array([5])
        return ratings, 0, 0

    n_pushes = len(responses)

    for response in responses:
        ratings.append((int(response[3][-1]), response[4]))

    t = np.linspace(blk_start_time, blk_start_time+duration-1, num=duration)
    r = np.zeros(duration)

    val = 5
    last = 0
    for rating in ratings:
        idx = np.where(t == rating[1])[0]

        # hack to save malformed data
        if len(idx) == 0:
            idx = last + 1
        logger.debug('last={} idx={} t={} rating={}'.format(last, idx, t, rating))

        r[last:idx] = val  # fill in all the values before the button push\
        val = rating[0]    # update the value to insert
        last = idx         # keep track of the last button push
    r[last:] = val         # fill in the tail end of the vector with the last recorded value

    return r, n_pushes, ratings

def find_column_data(blk_name, rating_file):
    """
    Returns the data from the column of specified file with the specified name.
    """
    # read in column names, convert to lowercase, compare with block name
    column_names = np.genfromtxt(rating_file, delimiter=',',
                                              dtype=str)[0].tolist()
    column_names = map(lambda x: x.lower(), column_names)
    column_number = np.where(np.array(column_names) == blk_name.lower())[0]

    # read in actor ratings from the selected column, strip nans
    column_data = np.genfromtxt(rating_file, delimiter=',',
                                              dtype=float, skip_header=2)

    # deal with a single value
    if len(np.shape(column_data)) == 1:
        column_data = column_data[column_number]
    # deal with a column of values
    elif len(np.shape(column_data)) == 2:
        column_data = column_data[:,column_number]
    # complain if the supplied rating_file is a dungparty
    else:
        logger.error('{} is not formatted properly!'.format(rating_file))
        raise ValueError
    # strip off NaN values
    column_data = column_data[np.isfinite(column_data)]

    return column_data

def match_lengths(a, b):
    """
    Matches the length of vector b to vector a using linear interpolation.
    """

    interp = interpolate.interp1d(np.linspace(0, len(b)-1, len(b)), b)
    b = interp(np.linspace(0, len(b)-1, len(a)))

    return b

def zscore(data):
    """
    z-transforms input vector. If this fails, return a vector of zeros.
    """
    datalength = len(data)
    try:
        data = (data - np.mean(data)) / np.std(data)
    except:
        data = np.zeros(datalength)

    return data

def r2z(data):
    """
    Fischer's r-to-z transform on a matrix (elementwise).
    """
    data = 0.5 * np.log( (1+data) / (1-data) )

    return data

def process_behav_data(log, assets, func_path, sub, trial_type, block_id):
    """
    This parses the behavioural log files for a given trial type (either
    'vid' for the empathic-accuracy videos, or 'cvid' for the circles task.

    First, the logs are parsed into list of 'picture', 'response', and 'video'
    events, as they contain a different number of columns and carry different
    information. The 'video' list is then used to find the start of each block.

    Within each block, this script goes about parsing the ratings made by
    the particpant using 'find_ratings'. The timing is extracted from the
    'response' list, and the actual rating is extracted from the 'picture'
    list.

    This is then compared with the hard-coded 'gold-standard' rating kept in
    a column of the specified .csv file. The lengths of these vectors are
    mached using linear interpolaton, and finally correlated. This correlation
    value is used as an amplitude modulator of the stimulus box-car. Another
    set of amplitude-modulated regressor of no interest is added using the
    number of button presses per run.

    The relationship between these ratings are written out to a .pdf file for
    visual inspection, however, the onsets, durations, and correlation values
    are only returned for the specified trial type. This should allow you to
    easily write out a GLM timing file with the onsets, lengths,
    correlations, and number of button-pushes split across trial types.
    """

    logger.debug('Processing behaviour log: {} for: {}'.format(sub,log))

    # make sure our trial type inputs are valid
    if trial_type not in ['vid', 'cvid']:
        logger.error(
                'trial_type input {} is incorrect: invalid vid or cvid'.format(
                    trial_type))
        raise ValueError

    try:
        pic, vid, mri_start = log_parser(log)
    except Exception, e:
        logger.error('Failed to parse log file: {}'.format(log))
        raise e

    logger.debug('Finding blocks')
    blocks, onsets = find_blocks(vid, mri_start)
    logger.debug('Found {} blocks'.format(len(blocks)))

    durations = []
    correlations = []
    onsets_used = []
    button_pushes = []
    all_ratings = []

    # format our output plot
    width, height = plt.figaspect(1.0/len(blocks))
    fig, axs = plt.subplots(1, len(blocks), figsize=(width, height*0.8))
    fig = plt.figure(figsize=(width, height))

    # Blocks seem to refer to videos within a block
    for i in np.linspace(0, len(blocks)-1, len(blocks)).astype(int).tolist():
        logger.debug('Processing block {}'.format(i))

        blk_start = blocks[i][0]
        blk_start_time = blocks[i][2]

        # block end is the beginning of the next trial
        try:
            blk_end = blocks[i+1][0]
        # unless we are on the final trial of the block, then we return None
        except:
            blk_end = None

        blk_name = blocks[i][1]

        gold_rate = find_column_data(blk_name, os.path.join(assets, 'EA-timing.csv'))
        duration = find_column_data(blk_name, os.path.join(assets, 'EA-vid-lengths.csv'))[0]

        logger.debug('Finding ratings for block {}'.format(i))

        subj_rate, n_pushes, ratings = find_ratings(pic, blk_start, blk_end, blk_start_time, duration*10000)

        logger.debug('Found {} ratings for {} events'.format(len(subj_rate), n_pushes))

        # interpolate the gold standard sample to match the subject sample
        logger.debug('Interpolating gold standard')
        if n_pushes != 0:
            gold_rate = match_lengths(subj_rate, gold_rate)
        else:
            subj_rate = np.repeat(5, len(gold_rate))

        # z-score both ratings
        logger.debug('Converting ratings to Z-scores')
        gold_rate = zscore(gold_rate)
        subj_rate = zscore(subj_rate)

        corr = np.corrcoef(subj_rate, gold_rate)[1][0]

        if np.isnan(corr):
            corr = 0  # this happens when we get no responses

        corr = r2z(corr) # z score correlations

        # add our ish to a kewl plot
        axs[i].plot(gold_rate, color='black', linewidth=2)
        axs[i].plot(subj_rate, color='red', linewidth=2)
        axs[i].set_title(blk_name + ': z(r) = ' + str(corr), size=10)
        axs[i].set_xlim((0,len(subj_rate)-1))
        axs[i].set_xlabel('TR')
        axs[i].set_xticklabels([])
        axs[i].set_ylim((-3, 3))
        if i == 0:
            axs[i].set_ylabel('Rating (z)')
        if i == len(blocks) -1:
            axs[i].legend(['Actor', 'Participant'], loc='best', fontsize=10, frameon=False)

        # skip the 'other' kind of task
        if trial_type == 'vid' and blocks[i][1][0] == 'c':
            continue

        elif trial_type == 'cvid' and blocks[i][1][0] == 'v':
            continue

        # otherwise, save the output vectors in seconds
        else:
            try:
                for r in ratings:
                    #collate the button push times and correct for mri start_time
                    # the correction should make them compatible with onsets_used
                            # appending ['new_value', 'time ms', 'block', 'vid_id']
                    all_ratings.append((r[0],r[1] - mri_start, block_id, blocks[i][1]))
            except TypeError:
                logger.warn('No ratings found for block {}'.format(i))
            onsets_used.append((blocks[i][1], onsets[i] - mri_start/10000.0, block_id))
            durations.append(duration.tolist())

            if type(corr) == int:
                correlations.append(corr)
            else:
                correlations.append(corr.tolist())
            # button pushes per minute (duration is in seconds)
            button_pushes.append(n_pushes / (duration.tolist() / 60.0))

    fig.suptitle(log, size=10)
    fig.set_tight_layout(True)
    fig.savefig('{func_path}/{sub}/{sub}_{logname}.pdf'.format(func_path=func_path, sub=sub, logname=os.path.basename(log)[:-4]))

    return onsets_used, durations, correlations, button_pushes, all_ratings

def move_files_from_local(local_outdir, outputdir):
    if not os.path.isdir(os.path.join(outputdir)):
        try:
            os.makedirs(os.path.join(outputdir))
        except IOError as e:
            msg = 'Failed to create the output dir: {} with excuse: {}'.format(outputdir, e.strerror)
            logger.error(msg)
            return(msg)

    logger.info('Moving data files...')
    try:
        root_src_dir = os.path.join(local_outdir)
        root_dst_dir = os.path.join(outputdir)

        logger.debug('From:{}, To:{}'.format(root_src_dir, root_dst_dir))
        for src_dir, dirs, files in os.walk(root_src_dir):
            dst_dir = src_dir.replace(root_src_dir, root_dst_dir)
            if not os.path.exists(dst_dir):
                os.makedirs(dst_dir)
            for filename in files:
                src_file = os.path.join(src_dir, filename)
                dst_file = os.path.join(dst_dir, filename)
                if os.path.exists(dst_file):
                    os.remove(dst_file)
                shutil.move(src_file, dst_file)
    except (IOError, OSError, shutil.Error) as e:
        logger.error('Failed to move files from {} on system {} to {} with excuse {}'.format(
            local_outdir, os.uname()[1], outputdir))


def main(local_outdir, arguments):
    global datadir
    study    = arguments['<study>']
    outputdir  = arguments['<outputdir>']
    assets     = arguments['<assets>']
    session    = arguments['<session>']

    walltime   = arguments['--walltime']
    dryrun     = arguments['--dryrun']
    logdir     = arguments['--logdir']



    if session:
        if not os.path.isdir(os.path.join(local_outdir, session)):
            try:
                os.makedirs(os.path.join(local_outdir, session))
            except IOError as e:
                msg = 'Failed to create the local output dir: {} with excuse: {}'.format(local_outdir, e.strerror)
                logger.error(msg)
                return(msg)
    else:
        local_outdir = outputdir

    if dm.scanid.is_phantom(session):
        msg = "Scan {} is a phantom. Skipping".format(session)
        logger.info(msg)
        return(msg)

    if not session:
        # process all sessions
        nii_path = os.path.join(datadir,'nii')
        sessions = dm.utils.get_subjects(nii_path)
        phantoms = dm.utils.get_phantoms(nii_path)
        valid_sessions = set(sessions) - set(phantoms)

        commands = []

        for session in valid_sessions:
            if check_complete(outputdir, session):
                msg = 'Session {} has already been analysed. Skipping.'.format(session)
                logger.info(msg)
            else:
                opts = (VERBOSE and ' --verbose' or '') + \
                       (DEBUG and ' --debug' or '') + \
                       (QUIET and ' --quiet' or '')
                opts = opts + ' --logdir {}'.format(arguments['--logdir'])

                commands.append(" ".join([__file__, opts, study,
                                          outputdir, session]))


        if commands:
            logger.info('Queuing up {} commands.'.format(len(commands)))
            logger.debug("queueing up the following commands:\n"+'\n'.join(commands))
            jobname = "dm_ea_behave_{}".format(time.strftime("%Y%m%d-%H%M%S"))
            fd, path = tempfile.mkstemp()
            os.write(fd, '\n'.join(commands))
            os.close(fd)

            rtn, out, err = dm.utils.run('qbatch -i --logdir {ld} -N {name} --walltime {wt} {cmds}'.format(
                ld = logdir,
                name = jobname,
                wt = walltime,
                cmds = path),dryrun = dryrun)

            if rtn != 0:
                logger.error("Job submission failed. Output follows.")
                logger.error("stdout: {}\nstderr: {}".format(out,err))
                sys.exit("Job submission failed.")
            #for command in commands:
            #    logger.info('Submitting job: {}, command:{}'.format(jobname,command))
            #    rtn, out, err = dm.utils.run('qsub -N {name} -V {cmd}'.format(
            #        name=jobname,
            #        cmd=command
            #    ),dryrun = dryrun)

            #    if rtn != 0:
            #        logger.error('Job:{} out:{}, err:{}'.format(jobname, out, err))

    # process a single session
    else:
        # check if session has already been processed
        if check_complete(outputdir, session):
            msg = 'Session {} has already been analysed. Skipping.'.format(session)
            logger.info(msg)
            return
        else:
            # create a temporary output directory on the local node

            try:
                resdirs = glob.glob(os.path.join(datadir, 'RESOURCES', session + '_??'))
                resources = []
                for resdir in resdirs:
                    resfiles = [os.path.join(dp, f) for
                                          dp, dn, fn in os.walk(resdir) for f in fn]
                    resources.extend(resfiles)

                logs = filter(lambda x: '.log' in x and 'UCLAEmpAcc' in x, resources)
                logs.sort()
            except:
                msg = 'No BEHAV data for {}.'.format(session)
                logger.error(msg)
                return(msg)

        if len(logs) != 3:
            msg = 'Did not find exactly 3 logs for {}.'.format(session)
            logger.error(msg)
            return(msg)

        for test_type in ['vid','cvid']:
            # extract all of the data from the logs
            on_all, dur_all, corr_all, push_all, timings_all = [], [], [], [], []

            try:
                logger.info('Parsing {} logfiles for session'.format(len(logs), session))
                for log in logs:
                    # extract the block id from the logfilename
                    block_id = os.path.splitext(os.path.basename(log))[0][-1]
                    on, dur, corr, push, timings = process_behav_data(log, assets, local_outdir, session, test_type, block_id)
                    on_all.extend(on)
                    dur_all.extend(dur)
                    corr_all.extend(corr)
                    push_all.extend(push)
                    timings_all.extend(timings)
            except Exception, e:
                msg = 'Failed to parse logs for {}, with {}.'.format(session,str(e))
                logger.error(msg)
                return(msg)

            timings_all = sorted(timings_all, key=lambda x: (x[2], x[3], x[1]))    # put the responses into order
            #on_all = sorted(on_all, key=lambda x:x[1])
            # write data to stimulus timing file for AFNI, and a QC csv
            try:
                logger.info('Writing stimulus data')
                # write each stimulus time:
                #         [start_time]*[amplitude],[buttonpushes]:[block_length]
                #         30*5,0.002:12

                # OFFSET 4 TRs == 8 Seconds!
                # on = on - 8.0
                f1 = open('{func_path}/{sub}/{sub}_{test}_block-times_ea.1D'.format(func_path=local_outdir, sub=session, test=test_type), 'wb') # stim timing file
                f2 = open('{func_path}/{sub}/{sub}_{test}_corr_push.csv'.format(func_path=local_outdir, sub=session, test=test_type), 'wb') # r values and num pushes / minute
                f3 = open('{func_path}/{sub}/{sub}_{test}_button-times.csv'.format(func_path=local_outdir, sub=session, test=test_type), 'wb') # button responses and timings
                f4 = open('{func_path}/{sub}/{sub}_{test}_vid-onsets.csv'.format(func_path=local_outdir, sub=session, test=test_type), 'wb') # button responses and timings
                f2.write('correlation,n-pushes-per-minute\n')
                f3.write('Block_ID,Video,Response,Timing\n')
                f4.write('Block_ID,Video, Onset\n')

                for i in range(len(on_all)):
                    f1.write('{o:.2f}*{r:.2f},{p}:{d:.2f} '.format(o=on_all[i][1]-8.0, r=corr_all[i], p=push_all[i], d=dur_all[i]))
                    f2.write('{r:.2f},{p}\n'.format(r=corr_all[i], p=push_all[i]))
                for timing in timings_all:
                    f3.write('{b},{v},{r},{t:.2f}\n'.format(b=timing[2], v=timing[3], r=timing[0], t=timing[1]))
                for onset in on_all:
                    f4.write('{b},{r},{t:.2f}\n'.format(b=onset[2], r=onset[0], t=onset[1]))
                f1.write('\n') # add newline at the end of each run (up to 3 runs.)
            except IOError as e:
                msg = 'Failed to open block_times & corr_push for {} with excuse {}'.format(session, e.strerror)
                logger.error(msg)
                return(msg)
            finally:
                f1.close()
                f2.close()
                f3.close()
                f4.close()
        return(0)


def generate_analysis_script(sub, func_path):
    """
    NOT YET WORKING.

    This writes the analysis script to replicate the methods in Harvey et al
    2013 Schizophrenia Bulletin. It expects timing files to exist.

    Briefly, this method uses the correlation between the empathic ratings of
    the participant and the actor from each video to generate an amplitude-
    modulated box-car model to be fit to each time-series. This model is
    convolved with an HRF, and is run alongside a standard boxcar. This allows
    us to detect regions that modulate their 'activation strength' with
    empathic accruacy, and those that generally track the watching of
    emotionally-valenced videos (but do not parametrically modulate).
    Since each video is of a different length, each block is encoded as such
    in the stimulus-timing file (all times in seconds):
        [start_time]*[amplitude]:[block_length]
        30*5:12
    See '-stim_times_AM2' in AFNI's 3dDeconvolve 'help' for more.
    """
    # first, determine input functional files
    niftis = filter(lambda x: 'nii.gz' in x and sub + '_func' in x, os.listdir(os.path.join(func_path, sub)))
    niftis.sort()

    input_data = ''

    for nifti in niftis:
        input_data = input_data + os.path.join(func_path, sub, nifti) + ' '

    # open up the master script, write common variables
    f = open('{func_path}/{sub}/{sub}_glm_1stlevel_cmd.sh'.format(func_path=func_path, sub=sub), 'wb')
    f.write("""#!/bin/bash
# Empathic accuracy GLM for {sub}.
3dDeconvolve \\
    -input {input_data} \\
    -mask {func_path}/{sub}/{sub}_anat_EPI_mask_MNI.nii.gz \\
    -ortvec {func_path}/{sub}/{sub}_motion.1D motion_paramaters \\
    -polort 4 \\
    -num_stimts 1 \\
    -local_times \\
    -jobs 8 \\
    -x1D {func_path}/{sub}/{sub}_glm_1stlevel_design.mat \\
    -stim_times_AM2 1 {func_path}/{sub}/{sub}_block-times_ea.1D \'dmBLOCK(1)\' \\
    -stim_label 1 empathic_accuracy \\
    -fitts {func_path}/{sub}/{sub}_glm_1stlevel_explained.nii.gz \\
    -errts {func_path}/{sub}/{sub}_glm_1stlevel_residuals.nii.gz \\
    -bucket {func_path}/{sub}/{sub}_glm_1stlevel.nii.gz \\
    -cbucket {func_path}/{sub}/{sub}_glm_1stlevel_coeffs.nii.gz \\
    -fout \\
    -tout \\
    -xjpeg {func_path}/{sub}/{sub}_glm_1stlevel_matrix.jpg
""".format(input_data=input_data, func_path=func_path, sub=sub))
    f.close()

if __name__=='__main__':

    arguments  = docopt(__doc__)

    VERBOSE      = arguments['--verbose']
    DEBUG        = arguments['--debug']
    QUIET        = arguments['--quiet']

    outputdir  = arguments['--output']
    session    = arguments['<session>']

    cfg = dm.config.config()
    try:
        cfg.set_study_config(study)
    except KeyError:
        msg = 'Invalid study:{}'.format(study)
        logger.error(msg)
        return(msg)

    # get the datadir from cfg
    if cfg.key_exists('study', ['PROJECTDIR']):
        datadir = cfg.study_config['PROJECTDIR'] \
                .replace('${DATMAN_PROJECTSDIR}',
                         cfg.site_config['SystemSettings'][cfg.system_name]['DATMAN_PROJECTSDIR'])
        datadir = os.path.join(datadir, 'data')
    else:
        msg = 'Failed to find datadir'
        logger.error(msg)
        return(msg)

    if not outputdir:
        outputdir = os.path.join(datadir, 'ea')
    # create a local tmpdir
    tmp_outdir = tempfile.mkdtemp()

    # Setup the logging
    if session:
        try:
            os.makedirs(os.path.join(tmp_outdir, session))
        except IOError:
            raise
        local_logpath = os.path.join(tmp_outdir, session, 'log.log')
    else:
        local_logpath = os.path.join(tmp_outdir,'log.log')

    fh = logging.FileHandler(local_logpath)
    ch = logging.StreamHandler()
    fh.setLevel(logging.ERROR)
    ch.setLevel(logging.WARN)

    if QUIET:
        ch.setLevel(logging.ERROR)

    if VERBOSE:
        ch.setLevel(logging.INFO)

    if DEBUG:
        ch.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)

    logger.addHandler(fh)
    logger.addHandler(ch)

    logger.info('Creating tempdir:{} on host:{}'.format(tmp_outdir, os.uname()[1]))
    ret = main(tmp_outdir, arguments)
    if ret == 0:
        move_files_from_local(tmp_outdir, outputdir)
    shutil.rmtree(tmp_outdir)
    sys.exit(ret)
